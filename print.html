<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js navy">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Home Operations</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="././mdbook-admonish.css">

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "navy";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('navy')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><li class="part-title">Welcome</li><li class="spacer"></li><li class="chapter-item expanded "><a href="introduction.html">Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">Basement Notes</li><li class="spacer"></li><li class="chapter-item expanded "><a href="notes/nas.html">NAS</a></li><li class="chapter-item expanded "><a href="notes/opnsense.html">Opnsense</a></li><li class="chapter-item expanded "><a href="notes/pikvm.html">PiKVM</a></li><li class="chapter-item expanded "><a href="notes/proxmox-considerations.html">Proxmox Considerations</a></li><li class="chapter-item expanded "><a href="notes/s3-buckets.html">S3 buckets</a></li><li class="chapter-item expanded "><a href="notes/secret-variations-with-flux.html">Secret variations with Flux</a></li><li class="chapter-item expanded "><a href="notes/yaml-madness.html">YAML Madness</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Home Operations</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/cfallwell/homelab-ops" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<div id="admonition-warning" class="admonition warning">
<div class="admonition-title">
<p>Warning</p>
<p><a class="admonition-anchor-link" href="introduction.html#admonition-warning"></a></p>
</div>
<div>
<p>These docs contain information that relates to my setup. They may or may not work for you.</p>
</div>
</div>
<hr />
<br />
<p>#yamllint disable
#Brokenlinks disable</p>
<h1 id="template-for-deploying-k3s-backed-by-flux"><a class="header" href="#template-for-deploying-k3s-backed-by-flux">Template for deploying k3s backed by Flux</a></h1>
<p>Highly opinionated template for deploying a single <a href="https://k3s.io">k3s</a> cluster with <a href="https://www.ansible.com">Ansible</a> and <a href="https://www.terraform.io">Terraform</a> backed by <a href="https://toolkit.fluxcd.io/">Flux</a> and <a href="https://toolkit.fluxcd.io/guides/mozilla-sops/">SOPS</a>.</p>
<p>The purpose here is to showcase how you can deploy an entire Kubernetes cluster and show it off to the world using the <a href="https://www.weave.works/blog/what-is-gitops-really">GitOps</a> tool <a href="https://toolkit.fluxcd.io/">Flux</a>. When completed, your Git repository will be driving the state of your Kubernetes cluster. In addition with the help of the <a href="https://github.com/ansible-collections/community.sops">Ansible</a>, <a href="https://github.com/carlpett/terraform-provider-sops">Terraform</a> and <a href="https://toolkit.fluxcd.io/guides/mozilla-sops/">Flux</a> SOPS integrations you'll be able to commit <a href="https://github.com/FiloSottile/age">Age</a> encrypted secrets to your public repo.</p>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<ul>
<li><a href="https://github.com/k8s-at-home/flux-cluster-template#-introduction">Introduction</a></li>
<li><a href="https://github.com/k8s-at-home/flux-cluster-template#-prerequisites">Prerequisites</a></li>
<li><a href="https://github.com/k8s-at-home/flux-cluster-template#-repository-structure">Repository structure</a></li>
<li><a href="https://github.com/k8s-at-home/flux-cluster-template#-lets-go">Lets go!</a></li>
<li><a href="https://github.com/k8s-at-home/flux-cluster-template#-post-installation">Post installation</a></li>
<li><a href="https://github.com/k8s-at-home/flux-cluster-template#-troubleshooting">Troubleshooting</a></li>
<li><a href="https://github.com/k8s-at-home/flux-cluster-template#-whats-next">What's next</a></li>
<li><a href="https://github.com/k8s-at-home/flux-cluster-template#-thanks">Thanks</a></li>
</ul>
<h2 id="-introduction"><a class="header" href="#-introduction">üëã Introduction</a></h2>
<p>The following components will be installed in your <a href="https://k3s.io/">k3s</a> cluster by default. Most are only included to get a minimum viable cluster up and running.</p>
<ul>
<li><a href="https://toolkit.fluxcd.io/">flux</a> - GitOps operator for managing Kubernetes clusters from a Git repository</li>
<li><a href="https://kube-vip.io/">kube-vip</a> - Load balancer for the Kubernetes control plane nodes</li>
<li><a href="https://metallb.universe.tf/">metallb</a> - Load balancer for Kubernetes services</li>
<li><a href="https://cert-manager.io/">cert-manager</a> - Operator to request SSL certificates and store them as Kubernetes resources</li>
<li><a href="https://www.tigera.io/project-calico/">calico</a> - Container networking interface for inter pod and service networking</li>
<li><a href="https://github.com/kubernetes-sigs/external-dns">external-dns</a> - Operator to publish DNS records to Cloudflare (and other providers) based on Kubernetes ingresses</li>
<li><a href="https://github.com/ori-edge/k8s_gateway">k8s_gateway</a> - DNS resolver that provides local DNS to your Kubernetes ingresses</li>
<li><a href="https://kubernetes.github.io/ingress-nginx/">ingress-nginx</a> - Kubernetes ingress controller used for a HTTP reverse proxy of Kubernetes ingresses</li>
<li><a href="https://github.com/rancher/local-path-provisioner">local-path-provisioner</a> - provision persistent local storage with Kubernetes</li>
</ul>
<p><em>Additional applications include <a href="https://github.com/toboshii/hajimari">hajimari</a>, <a href="https://github.com/tarampampam/error-pages">error-pages</a>, <a href="https://github.com/Ealenn/Echo-Server">echo-server</a>, <a href="https://github.com/rancher/system-upgrade-controller">system-upgrade-controller</a>, <a href="https://github.com/emberstack/kubernetes-reflector">reflector</a>, <a href="https://github.com/stakater/Reloader">reloader</a>, and <a href="https://github.com/weaveworks/kured">kured</a></em></p>
<p>For provisioning the following tools will be used:</p>
<ul>
<li><a href="https://getfedora.org/en/server/download/">Fedora 36 Server</a> - Universal operating system that supports running all kinds of home related workloads in Kubernetes and has a faster release cycle</li>
<li><a href="https://www.ansible.com">Ansible</a> - Provision Fedora Server and install k3s</li>
<li><a href="https://www.terraform.io">Terraform</a> - Provision an already existing Cloudflare domain and certain DNS records to be used with your k3s cluster</li>
</ul>
<h2 id="-prerequisites"><a class="header" href="#-prerequisites">üìù Prerequisites</a></h2>
<p><strong>Note:</strong> <em>This template has not been tested on cloud providers like AWS EC2, Hetzner, Scaleway etc... Those cloud offerings probably have a better way of provsioning a Kubernetes cluster and it's advisable to use those instead of the Ansible playbooks included here. This repository can still be tweaked for the GitOps/Flux portion if there's a cluster working in one those environments.</em></p>
<h3 id="-reading-material"><a class="header" href="#-reading-material">üìö Reading material</a></h3>
<ul>
<li><a href="https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/">Organizing Cluster Access Using kubeconfig Files</a></li>
</ul>
<h3 id="-systems"><a class="header" href="#-systems">üíª Systems</a></h3>
<ul>
<li>One or more nodes with a fresh install of <a href="https://getfedora.org/en/server/download/">Fedora Server 36</a>.
<ul>
<li>These nodes can be ARM64/AMD64 bare metal or VMs.</li>
<li>An odd number of control plane nodes, greater than or equal to 3 is required if deploying more than one control plane node.</li>
</ul>
</li>
<li>A <a href="https://www.cloudflare.com/">Cloudflare</a> account with a domain, this will be managed by Terraform and external-dns. You can <a href="https://www.cloudflare.com/products/registrar/">register new domains</a> directly thru Cloudflare.</li>
<li>Some experience in debugging problems and a positive attitude ;)</li>
</ul>
<p>üìç It is recommended to have 3 master nodes for a highly available control plane.</p>
<h3 id="-workstation-tools"><a class="header" href="#-workstation-tools">üîß Workstation Tools</a></h3>
<ol>
<li>
<p>Install the <strong>most recent versions</strong> of the following CLI tools on your workstation, if you are using <a href="https://brew.sh/">Homebrew</a> on MacOS or Linux skip to steps 3 and 4.</p>
<ul>
<li>
<p>Required: <a href="https://github.com/FiloSottile/age">age</a>, <a href="https://www.ansible.com">ansible</a>, <a href="https://toolkit.fluxcd.io/">flux</a>, <a href="https://github.com/go-task/task">go-task</a>, <a href="http://jodies.de/ipcalc">ipcalc</a>, <a href="https://stedolan.github.io/jq/">jq</a>, <a href="https://kubernetes.io/docs/tasks/tools/">kubectl</a>, <a href="https://github.com/pre-commit/pre-commit">pre-commit</a>, <a href="https://github.com/mozilla/sops">sops</a>, <a href="https://www.terraform.io">terraform</a>, <a href="https://github.com/mikefarah/yq">yq v4</a></p>
</li>
<li>
<p>Recommended: <a href="https://github.com/direnv/direnv">direnv</a>, <a href="https://helm.sh/">helm</a>, <a href="https://github.com/kubernetes-sigs/kustomize">kustomize</a>, <a href="https://github.com/prettier/prettier">prettier</a>, <a href="https://github.com/stern/stern">stern</a>, <a href="https://github.com/adrienverge/yamllint">yamllint</a></p>
</li>
</ul>
</li>
<li>
<p>This guide heavily relies on <a href="https://github.com/go-task/task">go-task</a> as a framework for setting things up. It is advised to learn and understand the commands it is running under the hood.</p>
</li>
<li>
<p>Install <a href="https://github.com/go-task/task">go-task</a> via Brew</p>
<pre><code class="language-sh">brew install go-task/tap/go-task
</code></pre>
</li>
<li>
<p>Install workstation dependencies via Brew</p>
<pre><code class="language-sh">task init
</code></pre>
</li>
</ol>
<h3 id="-pre-commit"><a class="header" href="#-pre-commit">‚ö†Ô∏è pre-commit</a></h3>
<p>It is advisable to install <a href="https://pre-commit.com/">pre-commit</a> and the pre-commit hooks that come with this repository.
<a href="https://github.com/k8s-at-home/sops-pre-commit">sops-pre-commit</a> will check to make sure you are not committing non-encrypted Kubernetes secrets to your repository.</p>
<ol>
<li>
<p>Enable Pre-Commit</p>
<pre><code class="language-sh">task precommit:init
</code></pre>
</li>
<li>
<p>Update Pre-Commit, though it will occasionally make mistakes, so verify its results.</p>
<pre><code class="language-sh">task precommit:update
</code></pre>
</li>
</ol>
<h2 id="-repository-structure"><a class="header" href="#-repository-structure">üìÇ Repository structure</a></h2>
<p>The Git repository contains the following directories under <code>cluster</code> and are ordered below by how Flux will apply them.</p>
<pre><code class="language-sh">üìÅ cluster      # k8s cluster defined as code
‚îú‚îÄüìÅ flux       # flux, gitops operator, loaded before everything
‚îú‚îÄüìÅ crds       # custom resources, loaded before üìÅ core and üìÅ apps
‚îú‚îÄüìÅ charts     # helm repos, loaded before üìÅ core and üìÅ apps
‚îú‚îÄüìÅ config     # cluster config, loaded before üìÅ core and üìÅ apps
‚îú‚îÄüìÅ core       # crucial apps, namespaced dir tree, loaded before üìÅ apps
‚îî‚îÄüìÅ apps       # regular apps, namespaced dir tree, loaded last
</code></pre>
<h2 id="-lets-go"><a class="header" href="#-lets-go">üöÄ Lets go</a></h2>
<p>Very first step will be to create a new repository by clicking the <strong>Use this template</strong> button on this page.</p>
<p>Clone the repo to you local workstation and <code>cd</code> into it.</p>
<p>üìç <strong>All of the below commands</strong> are run on your <strong>local</strong> workstation, <strong>not</strong> on any of your cluster nodes.</p>
<h3 id="-setting-up-age"><a class="header" href="#-setting-up-age">üîê Setting up Age</a></h3>
<p>üìç Here we will create a Age Private and Public key. Using <a href="https://github.com/mozilla/sops">SOPS</a> with <a href="https://github.com/FiloSottile/age">Age</a> allows us to encrypt secrets and use them in Ansible, Terraform and Flux.</p>
<ol>
<li>
<p>Create a Age Private / Public Key</p>
<pre><code class="language-sh">age-keygen -o age.agekey
</code></pre>
</li>
<li>
<p>Set up the directory for the Age key and move the Age file to it</p>
<pre><code class="language-sh">mkdir -p ~/.config/sops/age
mv age.agekey ~/.config/sops/age/keys.txt
</code></pre>
</li>
<li>
<p>Export the <code>SOPS_AGE_KEY_FILE</code> variable in your <code>bashrc</code>, <code>zshrc</code> or <code>config.fish</code> and source it, e.g.</p>
<pre><code class="language-sh">export SOPS_AGE_KEY_FILE=~/.config/sops/age/keys.txt
source ~/.bashrc
</code></pre>
</li>
<li>
<p>Fill out the Age public key in the <code>.config.env</code> under <code>BOOTSTRAP_AGE_PUBLIC_KEY</code>, <strong>note</strong> the public key should start with <code>age</code>...</p>
</li>
</ol>
<h3 id="-global-cloudflare-api-key"><a class="header" href="#-global-cloudflare-api-key">‚òÅÔ∏è Global Cloudflare API Key</a></h3>
<p>In order to use Terraform and <code>cert-manager</code> with the Cloudflare DNS challenge you will need to create a API key.</p>
<ol>
<li>
<p>Head over to Cloudflare and create a API key by going <a href="https://dash.cloudflare.com/profile/api-tokens">here</a>.</p>
</li>
<li>
<p>Under the <code>API Keys</code> section, create a global API Key.</p>
</li>
<li>
<p>Use the API Key in the configuration section below.</p>
</li>
</ol>
<p>üìç You may wish to update this later on to a Cloudflare <strong>API Token</strong> which can be scoped to certain resources. I do not recommend using a Cloudflare <strong>API Key</strong>, however for the purposes of this template it is easier getting started without having to define which scopes and resources are needed. For more information see the <a href="https://developers.cloudflare.com/api/">Cloudflare docs on API Keys and Tokens</a>.</p>
<h3 id="-configuration"><a class="header" href="#-configuration">üìÑ Configuration</a></h3>
<p>üìç The <code>.config.env</code> file contains necessary configuration that is needed by Ansible, Terraform and Flux.</p>
<ol>
<li>
<p>Copy the <code>.config.sample.env</code> to <code>.config.env</code> and start filling out all the environment variables.</p>
<p><strong>All are required</strong> unless otherwise noted in the comments.</p>
<pre><code class="language-sh">cp .config.sample.env .config.env
</code></pre>
</li>
<li>
<p>Once that is done, verify the configuration is correct by running:</p>
<pre><code class="language-sh">task verify
</code></pre>
</li>
<li>
<p>If you do not encounter any errors run start having the script wire up the templated files and place them where they need to be.</p>
<pre><code class="language-sh">task configure
</code></pre>
</li>
</ol>
<h3 id="-preparing-fedora-server-with-ansible"><a class="header" href="#-preparing-fedora-server-with-ansible">‚ö° Preparing Fedora Server with Ansible</a></h3>
<p>üìç Here we will be running a Ansible Playbook to prepare Fedora Server for running a Kubernetes cluster.</p>
<p>üìç Nodes are not security hardened by default, you can do this with <a href="https://github.com/dev-sec/ansible-collection-hardening">dev-sec/ansible-collection-hardening</a> or similar if it supports Fedora Server.</p>
<ol>
<li>
<p>Ensure you are able to SSH into your nodes from your workstation using a private SSH key <strong>without a passphrase</strong>. This is how Ansible is able to connect to your remote nodes.</p>
<p><a href="https://www.digitalocean.com/community/tutorials/how-to-configure-ssh-key-based-authentication-on-a-linux-server">How to configure SSH key-based authentication</a></p>
</li>
<li>
<p>Install the Ansible deps</p>
<pre><code class="language-sh">task ansible:init
</code></pre>
</li>
<li>
<p>Verify Ansible can view your config</p>
<pre><code class="language-sh">task ansible:list
</code></pre>
</li>
<li>
<p>Verify Ansible can ping your nodes</p>
<pre><code class="language-sh">task ansible:ping
</code></pre>
</li>
<li>
<p>Run the Fedora Server Ansible prepare playbook</p>
<pre><code class="language-sh">task ansible:prepare
</code></pre>
</li>
<li>
<p>Reboot the nodes</p>
<pre><code class="language-sh">task ansible:reboot
</code></pre>
</li>
</ol>
<h3 id="-installing-k3s-with-ansible"><a class="header" href="#-installing-k3s-with-ansible">‚õµ Installing k3s with Ansible</a></h3>
<p>üìç Here we will be running a Ansible Playbook to install <a href="https://k3s.io/">k3s</a> with <a href="https://galaxy.ansible.com/xanmanning/k3s">this</a> wonderful k3s Ansible galaxy role. After completion, Ansible will drop a <code>kubeconfig</code> in <code>./provision/kubeconfig</code> for use with interacting with your cluster with <code>kubectl</code>.</p>
<p>‚ò¢Ô∏è If you run into problems, you can run <code>task ansible:nuke</code> to destroy the k3s cluster and start over.</p>
<ol>
<li>
<p>Verify Ansible can view your config</p>
<pre><code class="language-sh">task ansible:list
</code></pre>
</li>
<li>
<p>Verify Ansible can ping your nodes</p>
<pre><code class="language-sh">task ansible:ping
</code></pre>
</li>
<li>
<p>Install k3s with Ansible</p>
<pre><code class="language-sh">task ansible:install
</code></pre>
</li>
<li>
<p>Verify the nodes are online</p>
<pre><code class="language-sh">task cluster:nodes
# NAME           STATUS   ROLES                       AGE     VERSION
# k8s-0          Ready    control-plane,master      4d20h   v1.21.5+k3s1
# k8s-1          Ready    worker                    4d20h   v1.21.5+k3s1
</code></pre>
</li>
</ol>
<h3 id="-configuring-cloudflare-dns-with-terraform"><a class="header" href="#-configuring-cloudflare-dns-with-terraform">‚òÅÔ∏è Configuring Cloudflare DNS with Terraform</a></h3>
<p>üìç Review the Terraform scripts under <code>./provision/terraform/cloudflare/</code> and make sure you understand what it's doing (no really review it).</p>
<p>If your domain already has existing DNS records <strong>be sure to export those DNS settings before you continue</strong>.</p>
<ol>
<li>
<p>Pull in the Terraform deps</p>
<pre><code class="language-sh">task terraform:init
</code></pre>
</li>
<li>
<p>Review the changes Terraform will make to your Cloudflare domain</p>
<pre><code class="language-sh">task terraform:plan
</code></pre>
</li>
<li>
<p>Have Terraform apply your Cloudflare settings</p>
<pre><code class="language-sh">task terraform:apply
</code></pre>
</li>
</ol>
<p>If Terraform was ran successfully you can log into Cloudflare and validate the DNS records are present.</p>
<p>The cluster application <a href="https://github.com/kubernetes-sigs/external-dns">external-dns</a> will be managing the rest of the DNS records you will need.</p>
<h3 id="-gitops-with-flux"><a class="header" href="#-gitops-with-flux">üîπ GitOps with Flux</a></h3>
<p>üìç Here we will be installing <a href="https://toolkit.fluxcd.io/">flux</a> after some quick bootstrap steps.</p>
<ol>
<li>
<p>Verify Flux can be installed</p>
<pre><code class="language-sh">task cluster:verify
# ‚ñ∫ checking prerequisites
# ‚úî kubectl 1.21.5 &gt;=1.18.0-0
# ‚úî Kubernetes 1.21.5+k3s1 &gt;=1.16.0-0
# ‚úî prerequisites checks passed
</code></pre>
</li>
<li>
<p>Push you changes to git</p>
<p>üìç <strong>Verify</strong> all the <code>*.sops.yaml</code> and <code>*.sops.yml</code> files under the <code>./cluster</code> and <code>./provision</code> folders are <strong>encrypted</strong> with SOPS</p>
<pre><code class="language-sh">git add -A
git commit -m &quot;Initial commit üöÄ&quot;
git push
</code></pre>
</li>
<li>
<p>Install Flux and sync the cluster to the Git repository</p>
<pre><code class="language-sh">task cluster:install
# namespace/flux-system configured
# customresourcedefinition.apiextensions.k8s.io/alerts.notification.toolkit.fluxcd.io created
</code></pre>
</li>
<li>
<p>Verify Flux components are running in the cluster</p>
<pre><code class="language-sh">task cluster:pods -- -n flux-system
# NAME                                       READY   STATUS    RESTARTS   AGE
# helm-controller-5bbd94c75-89sb4            1/1     Running   0          1h
# kustomize-controller-7b67b6b77d-nqc67      1/1     Running   0          1h
# notification-controller-7c46575844-k4bvr   1/1     Running   0          1h
# source-controller-7d6875bcb4-zqw9f         1/1     Running   0          1h
</code></pre>
</li>
</ol>
<h3 id="-verification-steps"><a class="header" href="#-verification-steps">üé§ Verification Steps</a></h3>
<p><em>Mic check, 1, 2</em> - In a few moments applications should be lighting up like a Christmas tree üéÑ</p>
<p>You are able to run all the commands below with one task</p>
<pre><code class="language-sh">task cluster:resources
</code></pre>
<ol>
<li>
<p>View the Flux Git Repositories</p>
<pre><code class="language-sh">task cluster:gitrepositories
</code></pre>
</li>
<li>
<p>View the Flux kustomizations</p>
<pre><code class="language-sh">task cluster:kustomizations
</code></pre>
</li>
<li>
<p>View all the Flux Helm Releases</p>
<pre><code class="language-sh">task cluster:helmreleases
</code></pre>
</li>
<li>
<p>View all the Flux Helm Repositories</p>
<pre><code class="language-sh">task cluster:helmrepositories
</code></pre>
</li>
<li>
<p>View all the Pods</p>
<pre><code class="language-sh">task cluster:pods
</code></pre>
</li>
<li>
<p>View all the certificates and certificate requests</p>
<pre><code class="language-sh">task cluster:certificates
</code></pre>
</li>
<li>
<p>View all the ingresses</p>
<pre><code class="language-sh">task cluster:ingresses
</code></pre>
</li>
</ol>
<p>üèÜ <strong>Congratulations</strong> if all goes smooth you'll have a Kubernetes cluster managed by Flux, your Git repository is driving the state of your cluster.</p>
<p>‚ò¢Ô∏è If you run into problems, you can run <code>task ansible:nuke</code> to destroy the k3s cluster and start over.</p>
<p>üß† Now it's time to pause and go get some coffee ‚òï because next is describing how DNS is handled.</p>
<h2 id="-post-installation"><a class="header" href="#-post-installation">üì£ Post installation</a></h2>
<h3 id="-dns"><a class="header" href="#-dns">üåê DNS</a></h3>
<p>üìç The <a href="https://github.com/kubernetes-sigs/external-dns">external-dns</a> application created in the <code>networking</code> namespace will handle creating public DNS records. By default, <code>echo-server</code> is the only public domain exposed on your Cloudflare domain. In order to make additional applications public you must set an ingress annotation like in the <code>HelmRelease</code> for <code>echo-server</code>. You do not need to use Terraform to create additional DNS records unless you need a record outside the purposes of your Kubernetes cluster (e.g. setting up MX records).</p>
<p><a href="https://github.com/ori-edge/k8s_gateway">k8s_gateway</a> is deployed on the IP choosen for <code>${BOOTSTRAP_METALLB_K8S_GATEWAY_ADDR}</code>. Inorder to test DNS you can point your clients DNS to the <code>${BOOTSTRAP_METALLB_K8S_GATEWAY_ADDR}</code> IP address and load <code>https://hajimari.${BOOTSTRAP_CLOUDFLARE_DOMAIN}</code> in your browser.</p>
<p>You can also try debugging with the command <code>dig</code>, e.g. <code>dig @${BOOTSTRAP_METALLB_K8S_GATEWAY_ADDR} hajimari.${BOOTSTRAP_CLOUDFLARE_DOMAIN}</code> and you should get a valid answer containing your <code>${BOOTSTRAP_METALLB_INGRESS_ADDR}</code> IP address.</p>
<p>If your router (or Pi-Hole, Adguard Home or whatever) supports conditional DNS forwarding (also know as split-horizon DNS) you may have DNS requests for <code>${SECRET_DOMAIN}</code> only point to the  <code>${BOOTSTRAP_METALLB_K8S_GATEWAY_ADDR}</code> IP address. This will ensure only DNS requests for <code>${SECRET_DOMAIN}</code> will only get routed to your <a href="https://github.com/ori-edge/k8s_gateway">k8s_gateway</a> service thus providing DNS resolution to your cluster applications/ingresses.</p>
<p>To access services from the outside world port forwarded <code>80</code> and <code>443</code> in your router to the <code>${BOOTSTRAP_METALLB_INGRESS_ADDR}</code> IP, in a few moments head over to your browser and you <em>should</em> be able to access <code>https://echo-server.${BOOTSTRAP_CLOUDFLARE_DOMAIN}</code> from a device outside your LAN.</p>
<p>Now if nothing is working, that is expected. This is DNS after all!</p>
<h3 id="-ssl"><a class="header" href="#-ssl">üîê SSL</a></h3>
<p>By default in this template Kubernetes ingresses are set to use the <a href="https://letsencrypt.org/docs/staging-environment/">Let's Encrypt Staging Environment</a>. This will hopefully reduce issues from ACME on requesting certificates until you are ready to use this in &quot;Production&quot;.</p>
<p>Once you have confirmed there are no issues requesting your certificates replace <code>letsencrypt-staging</code> with <code>letsencrypt-production</code> in your ingress annotations for <code>cert-manager.io/cluster-issuer</code></p>
<h3 id="-renovatebot"><a class="header" href="#-renovatebot">ü§ñ Renovatebot</a></h3>
<p><a href="https://www.mend.io/free-developer-tools/renovate/">Renovatebot</a> will scan your repository and offer PRs when it finds dependencies out of date. Common dependencies it will discover and update are Flux, Ansible Galaxy Roles, Terraform Providers, Kubernetes Helm Charts, Kubernetes Container Images, Pre-commit hooks updates, and more!</p>
<p>The base Renovate configuration provided in your repository can be view at <a href="https://github.com/k8s-at-home/flux-cluster-template/blob/main/.github/renovate.json5">.github/renovate.json5</a>. If you notice this only runs on weekends and you can <a href="https://docs.renovatebot.com/presets-schedule/">change the schedule to anything you want</a> or simply remove it.</p>
<p>To enable Renovate on your repository, click the 'Configure' button over at their <a href="https://github.com/apps/renovate">Github app page</a> and choose your repository. Over time Renovate will create PRs for out-of-date dependencies it finds. Any merged PRs that are in the cluster directory Flux will deploy.</p>
<h3 id="-github-webhook"><a class="header" href="#-github-webhook">ü™ù Github Webhook</a></h3>
<p>Flux is pull-based by design meaning it will periodically check your git repository for changes, using a webhook you can enable Flux to update your cluster on <code>git push</code>. In order to configure Github to send <code>push</code> events from your repository to the Flux webhook receiver you will need two things:</p>
<ol>
<li>
<p>Webhook URL - Your webhook receiver will be deployed on <code>https://flux-receiver.${BOOTSTRAP_CLOUDFLARE_DOMAIN}/hook/:hookId</code>. In order to find out your hook id you can run the following command:</p>
<pre><code class="language-sh">kubectl -n flux-system get receiver/github-receiver --kubeconfig=./provision/kubeconfig
# NAME              AGE    READY   STATUS
# github-receiver   6h8m   True    Receiver initialized with URL: /hook/12ebd1e363c641dc3c2e430ecf3cee2b3c7a5ac9e1234506f6f5f3ce1230e123
</code></pre>
<p>So if my domain was <code>k8s-at-home.com</code> the full url would look like this:</p>
<pre><code class="language-text">https://flux-receiver.k8s-at-home.com/hook/12ebd1e363c641dc3c2e430ecf3cee2b3c7a5ac9e1234506f6f5f3ce1230e123
</code></pre>
</li>
<li>
<p>Webhook secret - Your webhook secret can be found by decrypting the <code>secret.sops.yaml</code> using the following command:</p>
<pre><code class="language-sh">sops -d ./cluster/apps/flux-system/webhooks/github/secret.sops.yaml | yq .stringData.token
</code></pre>
<p><strong>Note:</strong> Don't forget to update the <code>BOOTSTRAP_FLUX_GITHUB_WEBHOOK_SECRET</code> variable in your <code>.config.env</code> file so it matches the generated secret if applicable</p>
</li>
</ol>
<p>Now that you have the webhook url and secret, it's time to set everything up on the Github repository side. Navigate to the settings of your repository on Github, under &quot;Settings/Webhooks&quot; press the &quot;Add webhook&quot; button. Fill in the webhook url and your secret.</p>
<h3 id="-storage"><a class="header" href="#-storage">üíæ Storage</a></h3>
<p>Rancher's <code>local-path-provisioner</code> is a great start for storage but soon you might find you need more features like replicated block storage, or to connect to a NFS/SMB/iSCSI server. Check out the projects below to read up more on some storage solutions that might work for you.</p>
<ul>
<li><a href="https://github.com/rook/rook">rook-ceph</a></li>
<li><a href="https://github.com/longhorn/longhorn">longhorn</a></li>
<li><a href="https://github.com/openebs/openebs">openebs</a></li>
<li><a href="https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner">nfs-subdir-external-provisioner</a></li>
<li><a href="https://github.com/democratic-csi/democratic-csi">democratic-csi</a></li>
<li><a href="https://github.com/kubernetes-csi/csi-driver-nfs">csi-driver-nfs</a></li>
<li><a href="https://github.com/SynologyOpenSource/synology-csi">synology-csi</a></li>
</ul>
<h3 id="-authenticate-flux-over-ssh"><a class="header" href="#-authenticate-flux-over-ssh">üîè Authenticate Flux over SSH</a></h3>
<p>Authenticating Flux to your git repository has a couple benefits like using a private git repository and/or using the Flux <a href="https://fluxcd.io/docs/components/image/">Image Automation Controllers</a>.</p>
<p>By default this template only works on a public GitHub repository, it is advised to keep your repository public.</p>
<p>The benefits of a public repository include:</p>
<ul>
<li>Debugging or asking for help, you can provide a link to a resource you are having issues with.</li>
<li>Adding a topic to your repository of <code>k8s-at-home</code> to be included in the <a href="https://whazor.github.io/k8s-at-home-search/">k8s-at-home-search</a>. This search helps people discover different configurations of Helm charts across others Flux based repositories.</li>
</ul>
<details>
  <summary>Expand to read guide on adding Flux SSH authentication</summary>
<ol>
<li>Generate new SSH key:
<pre><code class="language-sh">ssh-keygen -t ecdsa -b 521 -C &quot;github-deploy-key&quot; -f ./cluster/github-deploy-key -q -P &quot;&quot;
</code></pre>
</li>
<li>Paste public key in the deploy keys section of your repository settings</li>
<li>Create sops secret in <code>cluster/flux/flux-system/github-deploy-key.sops.yaml</code> with the contents of:
<pre><code class="language-yaml"># yamllint disable
apiVersion: v1
kind: Secret
metadata:
    name: github-deploy-key
    namespace: flux-system
stringData:
    # 3a. Contents of github-deploy-key
    identity:
    # 3b. Output of curl --silent https://api.github.com/meta | jq --raw-output '&quot;github.com &quot;+.ssh_keys[]'
    known_hosts: |
        github.com ssh-ed25519 ...
        github.com ecdsa-sha2-nistp256 ...
        github.com ssh-rsa ...
</code></pre>
</li>
<li>Encrypt secret:
<pre><code class="language-sh">sops --encrypt --in-place ./cluster/flux/flux-system/github-deploy-key.sops.yaml
</code></pre>
</li>
<li>Apply secret to cluster:
<pre><code class="language-sh">sops --decrypt cluster/flux/flux-system/github-deploy-key.sops.yaml | kubectl apply -f -
</code></pre>
</li>
<li>Update <code>cluster/flux/flux-system/flux-cluster.yaml</code>:
<pre><code class="language-yaml">---
apiVersion: source.toolkit.fluxcd.io/v1beta2
kind: GitRepository
metadata:
  name: flux-cluster
  namespace: flux-system
spec:
  interval: 10m
  # 6a: Change this to your user and repo names
  url: ssh://git@github.com/$user/$repo
  ref:
    branch: main
  secretRef:
    name: github-deploy-key
</code></pre>
</li>
<li>Commit and push changes</li>
<li>Force flux to reconcile your changes
<pre><code class="language-sh">task cluster:reconcile
</code></pre>
</li>
<li>Verify git repository is now using SSH:
<pre><code class="language-sh">task cluster:gitrepositories
</code></pre>
</li>
<li>Optionally set your repository to Private in your repository settings.</li>
</ol>
</details>
<h2 id="-troubleshooting"><a class="header" href="#-troubleshooting">üëâ Troubleshooting</a></h2>
<p>Our <a href="https://github.com/k8s-at-home/flux-cluster-template/wiki">wiki</a> (WIP, contributions welcome) is a good place to start troubleshooting issues. If that doesn't cover your issue, come join and say Hi in our <a href="https://discord.gg/k8s-at-home">Discord</a> server by starting a new thread in the #kubernetes support channel.</p>
<p>You may also open a issue on this GitHub repo or open a <a href="https://github.com/k8s-at-home/organization/discussions">discussion on GitHub</a>.</p>
<h2 id="-whats-next"><a class="header" href="#-whats-next">‚ùî What's next</a></h2>
<p>The world is your cluster, see below for important things you could work on adding.</p>
<p>Our Check out our <a href="https://github.com/k8s-at-home/flux-cluster-template/wiki">wiki</a> (WIP, contributions welcome) for more integrations!</p>
<h2 id="-thanks"><a class="header" href="#-thanks">ü§ù Thanks</a></h2>
<p>Big shout out to all the authors and contributors to the projects that we are using in this repository.</p>
<p>Community member @Whazor created <a href="https://whazor.github.io/k8s-at-home-search/">this website</a> as a creative way to search Helm Releases across GitHub. You may use it as a means to get ideas on how to configure an applications' Helm values.</p>
<p>Many people have shared their awesome repositories over at <a href="https://github.com/k8s-at-home/awesome-home-kubernetes">awesome-home-kubernetes</a>.</p>
<footer id="open-on-gh"><a href="https://github.com/cfallwell/homelab-ops/edit/main/docs/src/introduction.md">Edit this page on GitHub</a></footer><div style="break-before: page; page-break-before: always;"></div><h1 id="nas"><a class="header" href="#nas">NAS</a></h1>
<p>Outside of using <a href="https://github.com/ansible/ansible">Ansible</a> for configuring the OS, there are some manual steps I did to set it up.</p>
<h2 id="zfs"><a class="header" href="#zfs">ZFS</a></h2>
<h3 id="mirrored-zpool"><a class="header" href="#mirrored-zpool">Mirrored Zpool</a></h3>
<ol>
<li>
<p>Create initial pool and set configuration</p>
<pre><code class="language-sh">sudo zpool create -o ashift=12 -f eros mirror \
    /dev/disk/by-id/scsi-SATA_WDC_WD120EDGZ-11_9LHWA5KG \
    /dev/disk/by-id/scsi-SATA_WDC_WD120EMFZ-11_9MG0AHZA
sudo zfs set atime=off eros
sudo zfs set compression=lz4 eros
</code></pre>
</li>
<li>
<p>Attach more mirrors</p>
<pre><code class="language-sh">sudo zpool add eros mirror \
    /dev/disk/by-id/scsi-SATA_ST12000VN0007-2G_ZCH0B3D2 \
    /dev/disk/by-id/scsi-SATA_WDC_WD120EMFZ-11_X1G3B01L
</code></pre>
</li>
<li>
<p>Add spares</p>
<pre><code class="language-sh">sudo zpool add -f eros spare \
    /dev/disk/by-id/scsi-SATA_WDC_WD120EMFZ-11_QGGETR5T
</code></pre>
</li>
</ol>
<h3 id="datasets"><a class="header" href="#datasets">Datasets</a></h3>
<ol>
<li>
<p>Create datasets</p>
<pre><code class="language-sh">sudo zfs create nfs/Apps
sudo zfs create nfs/Apps/MinIO
sudo zfs create nfs/Media
</code></pre>
</li>
<li>
<p>Share dataset over NFS</p>
<pre><code class="language-sh">sudo zfs set \
    sharenfs=&quot;no_subtree_check,all_squash,anonuid=568,anongid=100,rw=@172.16.70.0/24,rw=@192.168.1.0/24,ro=192.168.150.21,ro=192.168.150.28&quot; \
    nfs/Media
sudo zfs set \
    sharenfs=&quot;no_subtree_check,all_squash,anonuid=568,anongid=100,rw=@172.16.70.0/24,rw=@192.168.1.0/24&quot; \
    nfs/Apps/MinIO
</code></pre>
</li>
<li>
<p>Dataset Permissions</p>
<pre><code class="language-sh">sudo chmod 770 /nfs/Media
sudo chown -R colin:users /nfs/Media
</code></pre>
</li>
</ol>
<h3 id="snapshots"><a class="header" href="#snapshots">Snapshots</a></h3>
<ol>
<li>
<p>Add or replace the file <code>/etc/sanoid/sanoid.conf</code></p>
<pre><code class="language-ini">[nfs/Media]
use_template = media

[template_media]
frequently = 0
hourly = 0
daily = 7
monthly = 0
yearly = 0
autosnap = yes
autoprune = yes
</code></pre>
</li>
<li>
<p>Start and enable sanoid</p>
<pre><code class="language-sh">sudo systemctl enable --now sanoid.timer
</code></pre>
</li>
<li>
<p>Give a local user access to a specific datasets snapshots</p>
<pre><code class="language-sh">sudo zfs allow -u jeff send,snapshot,hold nfs/Media
</code></pre>
</li>
</ol>
<h2 id="nfs"><a class="header" href="#nfs">NFS</a></h2>
<h3 id="local-nfs-shares"><a class="header" href="#local-nfs-shares">Local NFS Shares</a></h3>
<ol>
<li>
<p>Add or replace file <code>/etc/exports.d/local.exports</code></p>
<pre><code class="language-text">/share/PVCs 192.168.1.0/24(sec=sys,rw,no_subtree_check,all_squash,anonuid=568,anongid=100)
/share/PVCs 172.16.70.0/24(sec=sys,rw,no_subtree_check,all_squash,anonuid=568,anongid=100)
</code></pre>
</li>
<li>
<p>Dataset Permissions</p>
<pre><code class="language-sh">sudo chmod 770 /share/PVCs
sudo chown -R colin:users /share/PVCs
</code></pre>
</li>
<li>
<p>Reload exports</p>
<pre><code class="language-sh">sudo exportfs -arv
</code></pre>
</li>
</ol>
<h2 id="misc"><a class="header" href="#misc">Misc</a></h2>
<h3 id="badblocks"><a class="header" href="#badblocks">Badblocks</a></h3>
<div id="admonition-warning" class="admonition warning">
<div class="admonition-title">
<p>Warning</p>
<p><a class="admonition-anchor-link" href="notes/nas.html#admonition-warning"></a></p>
</div>
<div>
<p>This command is <strong>very destructive</strong> and should only be used to check for bad sectors</p>
</div>
</div>
<pre><code class="language-sh">sudo badblocks -b 4096 -wsv /dev/disk/by-id/scsi-SATA_ST12000VN0007-2G_ZJV01MC5
</code></pre>
<h3 id="lenovo-sa120"><a class="header" href="#lenovo-sa120">Lenovo SA120</a></h3>
<p>Due to the loudness of the fans, they can be adjusted by using <ins><a href="https://github.com/AndrewX192/lenovo-sa120-fanspeed-utility.git">AndrewX192/lenovo-sa120-fanspeed-utility</a></ins>.</p>
<footer id="open-on-gh"><a href="https://github.com/cfallwell/homelab-ops/edit/main/docs/src/notes/nas.md">Edit this page on GitHub</a></footer><div style="break-before: page; page-break-before: always;"></div><h1 id="opnsense"><a class="header" href="#opnsense">Opnsense</a></h1>
<h2 id="bgp"><a class="header" href="#bgp">BGP</a></h2>
<p>Instead of using Metallb for L2/L3 load balancer IPs I am using the Kubernetes Calico CNI with BGP which allows me to advertise load balancer IPs directly over BGP. This has some benefits like having equal cost multipath (ECMP) for scaled workloads in my cluster.</p>
<ol>
<li>Routing &gt; BPG | General
<ol>
<li><code>enable</code> = <code>true</code></li>
<li><code>BGP AS Number</code> = <code>64512</code></li>
<li><code>Network</code> = <code>172.16.70.0/24</code> (Subnet your Kubernetes nodes are on)</li>
<li>Save</li>
</ol>
</li>
<li>Routing &gt; BGP | Neighbors
<ul>
<li>Add a neighbor for each Kubernetes node
<ol>
<li><code>Enabled</code> = <code>true</code></li>
<li><code>Peer-IP</code> = <code>172.16.70.x</code> (Kubernetes Node IP)</li>
<li><code>Remote AS</code> = <code>64512</code></li>
<li><code>Update-Source Interface</code> = <code>HOME_SERVER</code> (VLAN of Kubernetes nodes)</li>
<li>Save</li>
<li>Continue adding neighbors until all your nodes are present</li>
</ol>
</li>
</ul>
</li>
<li>Routing &gt; General
<ol>
<li><code>Enable</code> = <code>true</code></li>
<li>Save</li>
</ol>
</li>
<li>System &gt; Settings &gt; Tunables
<ol>
<li>Add <code>net.route.multipath</code> and set the value to <code>1</code></li>
<li>Save</li>
</ol>
</li>
<li>Reboot</li>
<li>Verify
<ol>
<li>Routing &gt; Diagnostics | Summary</li>
</ol>
</li>
</ol>
<div id="admonition-warning" class="admonition warning">
<div class="admonition-title">
<p>Warning</p>
<p><a class="admonition-anchor-link" href="notes/opnsense.html#admonition-warning"></a></p>
</div>
<div>
<p>Without updating the configuration described in <strong>step 4</strong> the routes from a client will only take a <strong>single path to your Kubernetes workloads</strong> even if they are scaled to more than one.</p>
</div>
</div>
<h2 id="haproxy"><a class="header" href="#haproxy">HAProxy</a></h2>
<p>While kube-vip is very nice for having a API server ready to go and running in your cluster I had issues with mixing layer 2 and layer 3 between Calico in BGP and kube-vip using L2 ARP. You also cannot run Calico in BGP with kube-vip in BGP, they will fight and you will lose. Instead I choose to use Haproxy which you can install from the Opnsense Plugins.</p>
<ol>
<li>Services &gt; HAProxy | Real Servers
<ul>
<li>Add a server for each <strong>master node</strong> in your Kubernetes cluster
<ol>
<li><code>Enabled</code> = <code>true</code></li>
<li><code>Name or Prefix</code> = <code>k8s-apiserver-x</code></li>
<li><code>FQDN or IP</code> = <code>172.16.70.x</code></li>
<li><code>Port</code> = <code>6443</code></li>
<li><code>Verify SSL Certificate</code> = <code>false</code></li>
<li>Apply/Save</li>
<li>Continue adding servers until all your <strong>master nodes</strong> are present</li>
</ol>
</li>
</ul>
</li>
<li>Services &gt; HAProxy | Rules &amp; Checks &gt; Health Monitors
<ol>
<li><code>Name</code> = <code>k8s-apiserver-health</code></li>
<li><code>SSL preferences</code> = <code>Force SSL for health checks</code></li>
<li><code>Port to check</code> = <code>6443</code></li>
<li><code>HTTP method</code> = <code>GET</code></li>
<li><code>Request URI</code> = <code>/healthz</code></li>
<li><code>HTTP version</code> = <code>HTTP/1.1</code></li>
<li>Apply/Save</li>
</ol>
</li>
<li>Services &gt; HAProxy | Virtual Services &gt; Backend Pools
<ol>
<li><code>Enabled</code> = <code>true</code></li>
<li><code>Name</code> = <code>k8s-apiserver-be</code></li>
<li><code>Mode</code> = <code>TCP (Layer 4)</code></li>
<li><code>Servers</code> = <code>k8s-apiserver-x</code> ... (Add one for each server you created. Use TAB key to complete typing each server)</li>
<li><code>Source address</code> = <code>172.16.0.254</code> (Your Opnsense IP address)</li>
<li><code>Enable Health Checking</code> = <code>true</code></li>
<li><code>Health Monitor</code> = <code>k8s-apiserver-health</code></li>
<li>Apply/Save</li>
</ol>
</li>
<li>Services &gt; HAProxy | Virtual Services &gt; Public Services
<ol>
<li><code>Enabled</code> = <code>true</code></li>
<li><code>Name</code> = <code>k8s-apiserver-fe</code></li>
<li><code>Listen Addresses</code> = <code>172.16.0.254:6443</code> (Your Opnsense IP address. Use TAB key to complete typing a listen address)</li>
<li><code>Type</code> = <code>TCP</code></li>
<li><code>Default Backend Pool</code> = <code>k8s-apiserver-be</code></li>
<li>Apply/Save</li>
</ol>
</li>
<li>Services &gt; HAProxy | Settings &gt; Service
<ol>
<li><code>Enable HAProxy</code> = <code>true</code></li>
<li>Apply/Save</li>
</ol>
</li>
<li>Services &gt; HAProxy | Settings &gt; Global Parameters
<ol>
<li><code>Verify SSL Server Certificates</code> = <code>disable-verify</code></li>
<li>Apply/Save</li>
</ol>
</li>
<li>Services &gt; HAProxy | Settings &gt; Default Parameters
<ol>
<li><code>Client Timeout</code> = <code>4h</code></li>
<li><code>Connection Timeout</code> = <code>10s</code></li>
<li><code>Server Timeout</code> = <code>4h</code></li>
<li>Apply/Save</li>
</ol>
</li>
</ol>
<h2 id="receive-side-scaling-rss"><a class="header" href="#receive-side-scaling-rss">Receive Side Scaling (RSS)</a></h2>
<p>RSS is used to distribute packets over CPU cores using a hashing function ‚Äì either with support in the hardware which offloads the hashing for you, or in software. Click <ins><a href="https://forum.opnsense.org/index.php?topic=24409.0">here</a></ins> to learn more about it.</p>
<ol>
<li>System &gt; Settings &gt; Tunables
<ol>
<li>Add <code>net.inet.rss.enabled</code> and set the value to <code>1</code></li>
<li>Add <code>net.inet.rss.bits</code> and set to <code>2</code></li>
<li>Add <code>net.isr.dispatch</code> and set to <code>hybrid</code></li>
<li>Add <code>net.isr.bindthreads</code> and set to <code>1</code></li>
<li>Add <code>net.isr.maxthreads</code> and set to <code>-1</code></li>
<li>Save</li>
</ol>
</li>
<li>Reboot</li>
<li>Verify with <code>sudo netstat -Q</code>
<pre><code class="language-text">Configuration:
Setting                        Current        Limit
Thread count                         8            8
Default queue limit                256        10240
Dispatch policy                 hybrid          n/a
Threads bound to CPUs          enabled          n/a
</code></pre>
</li>
</ol>
<h2 id="syslog"><a class="header" href="#syslog">Syslog</a></h2>
<p>Firewall logs are being sent to <a href="https://github.com/vectordotdev/vector">Vector</a> which is running in my Kubernetes cluster. Vector is then shipping the logs to <a href="https://github.com/grafana/loki">Loki</a> which is also running in my cluster.</p>
<ol>
<li>System &gt; Settings &gt; Logging / targets
<ul>
<li>Add new logging target
<ol>
<li><code>Enabled</code> = <code>true</code></li>
<li><code>Transport</code> = <code>UDP(4)</code></li>
<li><code>Applications</code> = <code>filter (filterlog)</code></li>
<li><code>Hostname</code> = <code>192.168.69.111</code> (Loki's Load Balancer IP)</li>
<li><code>Port</code> = <code>5140</code></li>
<li><code>rfc5424</code> = <code>true</code></li>
<li>Save</li>
</ol>
</li>
</ul>
</li>
</ol>
<h2 id="smtp-relay"><a class="header" href="#smtp-relay">SMTP Relay</a></h2>
<p>To ease the use of application configuration I have a SMTP Relay running on Opnsense using the Postfix plugin. From applications deployed in my Kubernetes cluster, to my nas, to my printer, all use the same configuration for SMTP without authentication.</p>
<ol>
<li>System &gt; Services &gt; Postfix &gt; General
<ol>
<li><code>SMTP Client Security</code> = <code>encrypt</code></li>
<li><code>Smart Host</code> = <code>[smtp.fastmail.com]:465</code></li>
<li><code>Enable SMTP Authentication</code> = <code>true</code></li>
<li><code>Authentication Username</code> = <code>colin@&lt;email-domain&gt;</code></li>
<li><code>Authentication Password</code> = <code>&lt;app-password&gt;</code></li>
<li><code>Permit SASL Authenticated</code> = <code>false</code></li>
<li>Save</li>
</ol>
</li>
<li>System &gt; Services &gt; Postfix &gt; Domains
<ul>
<li>Add new domain
<ol>
<li><code>Domainname</code> = <code>&lt;email-domain&gt;</code></li>
<li><code>Destination</code> = <code>[smtp.fastmail.com]:465</code></li>
<li>Save</li>
</ol>
</li>
<li>Apply</li>
</ul>
</li>
<li>System &gt; Services &gt; Postfix &gt; Senders
<ul>
<li>Add new sender
<ol>
<li><code>Enabled</code> = <code>true</code></li>
<li><code>Sender Address</code> = <code>admin@&lt;email-domain&gt;</code></li>
<li>Save</li>
</ol>
</li>
<li>Apply</li>
</ul>
</li>
<li>Verify
<pre><code class="language-sh">swaks --server hassio.lab --port 25 --to &lt;email-address&gt; --from &lt;email-address&gt;
</code></pre>
</li>
</ol>
<footer id="open-on-gh"><a href="https://github.com/cfallwell/homelab-ops/edit/main/docs/src/notes/opnsense.md">Edit this page on GitHub</a></footer><div style="break-before: page; page-break-before: always;"></div><h1 id="pikvm"><a class="header" href="#pikvm">PiKVM</a></h1>
<h2 id="update-pikvm"><a class="header" href="#update-pikvm">Update PiKVM</a></h2>
<pre><code class="language-sh">rw; pacman -Syyu
reboot
</code></pre>
<h2 id="load-tesmart-kvm"><a class="header" href="#load-tesmart-kvm">Load TESmart KVM</a></h2>
<ol>
<li>
<p>Add or replace the file <code>/etc/kvmd/override.yaml</code></p>
<pre><code class="language-yaml">kvmd:
    gpio:
        drivers:
            tes:
                type: tesmart
                host: 172.16.0.2540
                port: 5000
        scheme:
            server0_led:
                driver: tes
                pin: 0
                mode: input
            server0_switch:
                driver: tes
                pin: 0
                mode: output
                switch: false
            server1_led:
                driver: tes
                pin: 1
                mode: input
            server1_switch:
                driver: tes
                pin: 1
                mode: output
                switch: false
            server2_led:
                driver: tes
                pin: 2
                mode: input
            server2_switch:
                driver: tes
                pin: 2
                mode: output
                switch: false
            server3_led:
                driver: tes
                pin: 3
                mode: input
            server3_switch:
                driver: tes
                pin: 3
                mode: output
                switch: false
            server4_led:
                driver: tes
                pin: 4
                mode: input
            server4_switch:
                driver: tes
                pin: 4
                mode: output
                switch: false
            server5_led:
                driver: tes
                pin: 5
                mode: input
            server5_switch:
                driver: tes
                pin: 5
                mode: output
                switch: false
            server6_led:
                driver: tes
                pin: 6
                mode: input
            server6_switch:
                driver: tes
                pin: 6
                mode: output
                switch: false
            server7_led:
                driver: tes
                pin: 7
                mode: input
            server7_switch:
                driver: tes
                pin: 7
                mode: output
                switch: false
        view:
            table:
                - [&quot;TESMART Switch&quot;]
                - []
                - [&quot;#device-0&quot;, server0_led, server0_switch|Switch]
                - [&quot;#device-1&quot;, server1_led, server1_switch|Switch]
                - [&quot;#device-2&quot;, server2_led, server2_switch|Switch]
                - [&quot;#device-3&quot;, server3_led, server3_switch|Switch]
                - [&quot;#device-4&quot;, server4_led, server4_switch|Switch]
                - [&quot;#device-5&quot;, server5_led, server5_switch|Switch]
                - [&quot;#device-6&quot;, server6_led, server6_switch|Switch]
                - [&quot;#device-7&quot;, server7_led, server7_switch|Switch]
</code></pre>
</li>
<li>
<p>Restart kvmd</p>
<pre><code class="language-sh">systemctl restart kvmd.service
</code></pre>
</li>
</ol>
<h2 id="load-custom-edid"><a class="header" href="#load-custom-edid">Load Custom EDID</a></h2>
<ol>
<li>
<p>Add or replace the file <code>/etc/kvmd/tc358743-edid.hex</code></p>
<pre><code class="language-text">00FFFFFFFFFFFF0052628888008888881C150103800000780AEE91A3544C99260F505425400001000100010001000100010001010101D32C80A070381A403020350040442100001E7E1D00A0500019403020370080001000001E000000FC0050492D4B564D20566964656F0A000000FD00323D0F2E0F000000000000000001C402030400DE0D20A03058122030203400F0B400000018E01500A04000163030203400000000000018B41400A050D011203020350080D810000018AB22A0A050841A3030203600B00E1100001800000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000045
</code></pre>
</li>
<li>
<p>Restart kvmd</p>
<pre><code class="language-sh">systemctl restart kvmd.service
</code></pre>
</li>
</ol>
<h2 id="disable-ssl"><a class="header" href="#disable-ssl">Disable SSL</a></h2>
<ol>
<li>
<p>Add or replace the file <code>/etc/kvmd/nginx/nginx.conf</code></p>
<pre><code class="language-nginx">worker_processes 4;

error_log stderr;

include /usr/share/kvmd/extras/*/nginx.ctx-main.conf;

events {
    worker_connections 1024;
    use epoll;
    multi_accept on;
}

http {
    types_hash_max_size 4096;
    server_names_hash_bucket_size 128;

    access_log off;

    include /etc/kvmd/nginx/mime-types.conf;
    default_type application/octet-stream;
    charset utf-8;

    sendfile on;
    tcp_nodelay on;
    tcp_nopush on;
    keepalive_timeout 10;
    client_max_body_size 4k;

    client_body_temp_path    /tmp/kvmd-nginx/client_body_temp;
    fastcgi_temp_path        /tmp/kvmd-nginx/fastcgi_temp;
    proxy_temp_path            /tmp/kvmd-nginx/proxy_temp;
    scgi_temp_path            /tmp/kvmd-nginx/scgi_temp;
    uwsgi_temp_path            /tmp/kvmd-nginx/uwsgi_temp;

    include /etc/kvmd/nginx/kvmd.ctx-http.conf;
    include /usr/share/kvmd/extras/*/nginx.ctx-http.conf;

    server {
        listen 80;
        listen [::]:80;
        server_name localhost;
        include /etc/kvmd/nginx/kvmd.ctx-server.conf;
        include /usr/share/kvmd/extras/*/nginx.ctx-server.conf;
    }
}
</code></pre>
</li>
<li>
<p>Restart kvmd-nginx</p>
<pre><code class="language-sh">systemctl restart kvmd-nginx.service
</code></pre>
</li>
</ol>
<h2 id="monitoring"><a class="header" href="#monitoring">Monitoring</a></h2>
<h3 id="install-node-exporter"><a class="header" href="#install-node-exporter">Install node-exporter</a></h3>
<pre><code class="language-sh">pacman -S prometheus-node-exporter
systemctl enable --now prometheus-node-exporter
</code></pre>
<h3 id="install-promtail"><a class="header" href="#install-promtail">Install promtail</a></h3>
<ol>
<li>
<p>Install promtail</p>
<pre><code class="language-sh">pacman -S promtail
systemctl enable promtail
</code></pre>
</li>
<li>
<p>Override the promtail systemd service</p>
<pre><code class="language-sh">mkdir -p /etc/systemd/system/promtail.service.d/
cat &gt;/etc/systemd/system/promtail.service.d/override.conf &lt;&lt;EOL
[Service]
Type=simple
ExecStart=
ExecStart=/usr/bin/promtail -config.file /etc/loki/promtail.yaml
EOL
</code></pre>
</li>
<li>
<p>Add or replace the file <code>/etc/loki/promtail.yaml</code></p>
<pre><code class="language-yaml">server:
  log_level: info
  disable: true

client:
  url: &quot;https://loki.$domain/loki/api/v1/push&quot;

positions:
  filename: /tmp/positions.yaml

scrape_configs:
  - job_name: journal
    journal:
      path: /run/log/journal
      max_age: 12h
      labels:
        job: systemd-journal
    relabel_configs:
      - source_labels: [&quot;__journal__systemd_unit&quot;]
        target_label: unit
      - source_labels: [&quot;__journal__hostname&quot;]
        target_label: hostname
</code></pre>
</li>
<li>
<p>Start promtail</p>
<pre><code class="language-sh">systemctl daemon-reload
systemctl start promtail.service
</code></pre>
</li>
</ol>
<footer id="open-on-gh"><a href="https://github.com/cfallwell/homelab-ops/edit/main/docs/src/notes/pikvm.md">Edit this page on GitHub</a></footer><div style="break-before: page; page-break-before: always;"></div><h1 id="proxmox-considerations"><a class="header" href="#proxmox-considerations">Proxmox Considerations</a></h1>
<p>I am using bare metal nodes but here's some considerations when using Kubernetes on Proxmox. These are just my opinions gathered from experiance I've witnessed first or second hand. I will always <strong>advocate for bare metal Kubernetes</strong> due to the overhead of VMs and disk concerns, however following along below will net you a very stable Kubernetes cluster on PVE.</p>
<div id="admonition-warning" class="admonition warning">
<div class="admonition-title">
<p>Warning</p>
<p><a class="admonition-anchor-link" href="notes/proxmox-considerations.html#admonition-warning"></a></p>
</div>
<div>
<p>Preface: etcd <strong>needs 3 master/control plane nodes for quorum</strong> also it is really read/write intensive and requires low iops/latency. With using the same disk for all master nodes and due to the way etcd works anytime a commit happens to etcd (which is probably hundreds of times per second), it will flood the same filesystem with 3x the amount of reads and writes</p>
<p>Now if you layer on Longhorn or rook-ceph to the <strong>same</strong> filesystem you are just asking for trouble, because that is also replicated.</p>
</div>
</div>
<h2 id="single-node-pve-cluster"><a class="header" href="#single-node-pve-cluster">Single Node PVE Cluster</a></h2>
<ol>
<li>Use physical separate disks used for the PVE install, k8s VMs and Longhorn/rook-ceph</li>
<li>Don't put k8s VMs or Longhorn/rook-ceph on HDDs, only use SSDs or NVMe</li>
<li>Use k3s with a single master node (4CPU/8GB RAM/50GB disk) that is using sqlite instead of etcd and taint it.</li>
<li>Use as many worker nodes as you want but start with 3 and add more later on if you need them.</li>
<li>Consider using <ins><a href="https://github.com/rancher/local-path-provisioner">local-path-provisioner</a></ins> over Longhorn or rook-ceph if you aren't able physically separate the disks.</li>
</ol>
<h2 id="dual-node-pve-cluster"><a class="header" href="#dual-node-pve-cluster">Dual node PVE Cluster</a></h2>
<p>Buy another node for your PVE cluster or refer to Single Node PVE Cluster, <em>however if you must...</em></p>
<ol>
<li>Use k3s with a dual master nodes (2vCPU/8GB RAM/50GB disk each) that is using postgresql/mariadb/mysql (in that order) instead of etcd.</li>
<li>Put the postgresql/mysql/mariadb database on a VM on your first PVE cluster. However, without some architecting this means that your cluster store is not highly available and is a single point of failure.</li>
<li>Evenly spread out your k8s masters and workers across each PVE node
<ul>
<li>In a 2 master/3 worker setup put one master on each PVE node and try to even out the workers on each PVE node.</li>
</ul>
</li>
<li>Consider using <ins><a href="https://github.com/rancher/local-path-provisioner">local-path-provisioner</a></ins> for application config data over Longhorn or rook-ceph if you aren't able physically separate the disks between Proxmox, VMs and Longhorn/rook-ceph.</li>
</ol>
<h2 id="tripe-node-pve-cluster"><a class="header" href="#tripe-node-pve-cluster">Tripe node PVE Cluster</a></h2>
<ol>
<li>Use physical separate disks used for the PVE install, k8s VMs and Longhorn</li>
<li>Don't put k8s VMs or Longhorn on HDDs, only use SSDs or NVMe</li>
<li>Evenly spread out your k8s masters and workers across each PVE node
<ul>
<li>In a 3 master/3 worker setup put one master on each PVE node and one worker on each PVE node.</li>
</ul>
</li>
<li>Instead of Longhorn, consider setting up a <ins><a href="https://pve.proxmox.com/wiki/Deploy_Hyper-Converged_Ceph_Cluster">Ceph cluster on your PVE nodes</a></ins> and use <ins><a href="https://rook.io/docs/rook/v1.10/CRDs/Cluster/external-cluster/">Rook to consume it</a></ins> for stateful applications. Due to the way Ceph works in this scenerio, it is fine to use HDDs over SSDs or NVMe here.</li>
</ol>
<footer id="open-on-gh"><a href="https://github.com/cfallwell/homelab-ops/edit/main/docs/src/notes/proxmox-considerations.md">Edit this page on GitHub</a></footer><div style="break-before: page; page-break-before: always;"></div><h1 id="s3-buckets"><a class="header" href="#s3-buckets">S3 buckets</a></h1>
<p>Alternatively creating s3 buckets can be automated with Terraform.</p>
<h2 id="b2"><a class="header" href="#b2">b2</a></h2>
<div id="admonition-info" class="admonition info">
<div class="admonition-title">
<p>Info</p>
<p><a class="admonition-anchor-link" href="notes/s3-buckets.html#admonition-info"></a></p>
</div>
<div>
<p>This requires installing the Backblaze <code>b2</code> CLI tool</p>
</div>
</div>
<h3 id="creating-a-bucket"><a class="header" href="#creating-a-bucket">Creating a bucket</a></h3>
<ol>
<li>
<p>Create master <code>key-id</code> and <code>key</code> on <ins><a href="https://secure.backblaze.com/app_keys.htm">Account &gt; App Keys</a></ins></p>
</li>
<li>
<p>Export settings</p>
<pre><code class="language-sh">export B2_APPLICATION_KEY_ID=&quot;&lt;key-id&gt;&quot;
export B2_APPLICATION_KEY=&quot;&lt;key&gt;&quot;
export B2_BUCKET_NAME=&quot;&lt;bucket-name&gt;&quot;
</code></pre>
</li>
<li>
<p>Create the bucket</p>
<pre><code class="language-sh">b2 create-bucket &quot;${B2_BUCKET_NAME}&quot; allPrivate \
  --defaultServerSideEncryption &quot;SSE-B2&quot;  \
  --lifecycleRules '[{&quot;daysFromHidingToDeleting&quot;: 1,&quot;daysFromUploadingToHiding&quot;: null,&quot;fileNamePrefix&quot;: &quot;&quot;}]'
</code></pre>
</li>
<li>
<p>Create the bucket username and password</p>
<pre><code class="language-sh">b2 create-key --bucket &quot;${B2_BUCKET_NAME}&quot; &quot;${B2_BUCKET_NAME}&quot; \
  listBuckets,readBuckets,listFiles,readFiles,writeFiles,readBucketEncryption,readBucketReplications,readBucketRetentions,readFileRetentions,writeFileRetentions,readFileLegalHolds
</code></pre>
</li>
</ol>
<h2 id="minio"><a class="header" href="#minio">Minio</a></h2>
<div id="admonition-info-1" class="admonition info">
<div class="admonition-title">
<p>Info</p>
<p><a class="admonition-anchor-link" href="notes/s3-buckets.html#admonition-info-1"></a></p>
</div>
<div>
<p>This requires installing the Minio <code>mc</code> CLI tool</p>
</div>
</div>
<h3 id="creating-a-bucket-1"><a class="header" href="#creating-a-bucket-1">Creating a Bucket</a></h3>
<ol>
<li>
<p>Create the Minio CLI configuration file (<code>~/.mc/config.json</code>)</p>
<pre><code class="language-sh">mc alias set minio &quot;https://s3.&lt;domain&gt;.&lt;tld&gt;&quot; &quot;&lt;access-key&gt;&quot; &quot;&lt;secret-key&gt;&quot;
</code></pre>
</li>
<li>
<p>Export settings</p>
<pre><code class="language-sh">export BUCKET_NAME=&quot;&lt;bucket-name&gt;&quot; # also used for the bucket username
export BUCKET_PASSWORD=&quot;$(openssl rand -hex 20)&quot;
echo $BUCKET_PASSWORD
</code></pre>
</li>
<li>
<p>Create the bucket username and password</p>
<pre><code class="language-sh">mc admin user add minio &quot;${BUCKET_NAME}&quot; &quot;${BUCKET_PASSWORD}&quot;
</code></pre>
</li>
<li>
<p>Create the bucket</p>
<pre><code class="language-sh">mc mb &quot;minio/${BUCKET_NAME}&quot;
</code></pre>
</li>
<li>
<p>Create the user policy document</p>
<pre><code class="language-sh">cat &lt;&lt;EOF &gt; /tmp/user-policy.json
{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Action&quot;: [
                &quot;s3:ListBucket&quot;,
                &quot;s3:PutObject&quot;,
                &quot;s3:GetObject&quot;,
                &quot;s3:DeleteObject&quot;
            ],
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Resource&quot;: [&quot;arn:aws:s3:::${BUCKET_NAME}/*&quot;, &quot;arn:aws:s3:::${BUCKET_NAME}&quot;],
            &quot;Sid&quot;: &quot;&quot;
        }
    ]
}
EOF
</code></pre>
</li>
<li>
<p>Apply the bucket policies</p>
<pre><code class="language-sh">mc admin policy add minio &quot;${BUCKET_NAME}-private&quot; /tmp/user-policy.json
</code></pre>
</li>
<li>
<p>Associate private policy with the user</p>
<pre><code class="language-sh">mc admin policy set minio &quot;${BUCKET_NAME}-private&quot; &quot;user=${BUCKET_NAME}&quot;
</code></pre>
</li>
</ol>
<h4 id="allow-public-access-to-certain-objects-in-the-bucket"><a class="header" href="#allow-public-access-to-certain-objects-in-the-bucket">Allow public access to certain objects in the bucket</a></h4>
<div id="admonition-info-2" class="admonition info">
<div class="admonition-title">
<p>Info</p>
<p><a class="admonition-anchor-link" href="notes/s3-buckets.html#admonition-info-2"></a></p>
</div>
<div>
<p>This step is optional and not needed unless you want to make certain objects public to the internet</p>
</div>
</div>
<ol>
<li>
<p>Create the bucket policy document and update the folders that should be public</p>
<pre><code class="language-sh">cat &lt;&lt;EOF &gt; /tmp/bucket-policy.json
{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Principal&quot;: {
                &quot;AWS&quot;: [
                    &quot;*&quot;
                ]
            },
            &quot;Action&quot;: [
                &quot;s3:GetBucketLocation&quot;
            ],
            &quot;Resource&quot;: [
                &quot;arn:aws:s3:::${BUCKET_NAME}&quot;
            ]
        },
        {
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Principal&quot;: {
                &quot;AWS&quot;: [
                    &quot;*&quot;
                ]
            },
            &quot;Action&quot;: [
                &quot;s3:ListBucket&quot;
            ],
            &quot;Resource&quot;: [
                &quot;arn:aws:s3:::${BUCKET_NAME}&quot;
            ],
            &quot;Condition&quot;: {
                &quot;StringEquals&quot;: {
                    &quot;s3:prefix&quot;: [
                        &quot;avatars&quot;,
                        &quot;public&quot;
                    ]
                }
            }
        },
        {
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Principal&quot;: {
                &quot;AWS&quot;: [
                    &quot;*&quot;
                ]
            },
            &quot;Action&quot;: [
                &quot;s3:GetObject&quot;
            ],
            &quot;Resource&quot;: [
                &quot;arn:aws:s3:::${BUCKET_NAME}/avatars*&quot;,
                &quot;arn:aws:s3:::${BUCKET_NAME}/public*&quot;
            ]
        }
    ]
}
EOF
</code></pre>
</li>
<li>
<p>Associate public policy with the bucket</p>
<pre><code class="language-sh">mc anonymous set-json /tmp/bucket-policy.json &quot;minio/${BUCKET_NAME}&quot;
</code></pre>
</li>
</ol>
<h3 id="sharing-an-object-in-a-bucket"><a class="header" href="#sharing-an-object-in-a-bucket">Sharing an object in a bucket</a></h3>
<pre><code class="language-sh">mc share download --expire=7d &quot;minio/&lt;bucket-name&gt;/&lt;file&gt;.&lt;ext&gt;&quot; --json  | jq -r .share | pbcopy
</code></pre>
<footer id="open-on-gh"><a href="https://github.com/cfallwell/homelab-ops/edit/main/docs/src/notes/s3-buckets.md">Edit this page on GitHub</a></footer><div style="break-before: page; page-break-before: always;"></div><h1 id="secret-variations-with-flux"><a class="header" href="#secret-variations-with-flux">Secret variations with Flux</a></h1>
<p>There are several different ways to utilize Kubernetes secrets when using <ins><a href="https://fluxcd.io/">Flux</a></ins> and <ins><a href="https://github.com/mozilla/sops">SOPS</a></ins>, here‚Äôs a breakdown of some common methods.</p>
<p><em>I will not be covering how to integrate SOPS into Flux for that be sure to check out the <ins><a href="https://fluxcd.io/docs/guides/mozilla-sops/">Flux documentation on integrating SOPS</a></ins></em></p>
<h2 id="example-secret"><a class="header" href="#example-secret">Example Secret</a></h2>
<div id="admonition-info" class="admonition info">
<div class="admonition-title">
<p>Info</p>
<p><a class="admonition-anchor-link" href="notes/secret-variations-with-flux.html#admonition-info"></a></p>
</div>
<div>
<p>The three following methods will use this secret as an example.</p>
</div>
</div>
<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: application-secret
  namespace: default
stringData:
  AWESOME_SECRET: &quot;SUPER SECRET VALUE&quot;
</code></pre>
<h3 id="method-1-envfrom"><a class="header" href="#method-1-envfrom">Method 1: <code>envFrom</code></a></h3>
<blockquote>
<p><em>Use <code>envFrom</code> in a deployment or a Helm chart that supports the setting, this will pass all secret items from the secret into the containers environment.</em></p>
</blockquote>
<pre><code class="language-yaml">envFrom:
  - secretRef:
      name: application-secret
</code></pre>
<div id="admonition-example" class="admonition example">
<div class="admonition-title">
<p>Example</p>
<p><a class="admonition-anchor-link" href="notes/secret-variations-with-flux.html#admonition-example"></a></p>
</div>
<div>
<p>View example <ins><a href="https://ln.intra.falhalla.com/ngLju">Helm Release</a></ins> and corresponding <ins><a href="https://ln.intra.falhalla.com/ULgnl">Secret</a></ins>.</p>
</div>
</div>
<h3 id="method-2-envvaluefrom"><a class="header" href="#method-2-envvaluefrom">Method 2: <code>env.valueFrom</code></a></h3>
<blockquote>
<p><em>Similar to the above but it's possible with <code>env</code> to pick an item from a secret.</em></p>
</blockquote>
<pre><code class="language-yaml">env:
  - name: WAY_COOLER_ENV_VARIABLE
    valueFrom:
      secretKeyRef:
        name: application-secret
        key: AWESOME_SECRET
</code></pre>
<div id="admonition-example-1" class="admonition example">
<div class="admonition-title">
<p>Example</p>
<p><a class="admonition-anchor-link" href="notes/secret-variations-with-flux.html#admonition-example-1"></a></p>
</div>
<div>
<p>View example <ins><a href="https://ln.intra.falhalla.com/0lbMT">Helm Release</a></ins> and corresponding <ins><a href="https://ln.intra.falhalla.com/KYjhP">Secret</a></ins>.</p>
</div>
</div>
<h3 id="method-3-specvaluesfrom"><a class="header" href="#method-3-specvaluesfrom">Method 3: <code>spec.valuesFrom</code></a></h3>
<blockquote>
<p><em>The Flux HelmRelease option <code>valuesFrom</code> can inject a secret item into the Helm values of a <code>HelmRelease</code></em></p>
<ul>
<li><em>Does not work with merging array values</em></li>
<li><em>Care needed with keys that contain dot notation in the name</em></li>
</ul>
</blockquote>
<pre><code class="language-yaml">valuesFrom:
  - targetPath: config.&quot;admin\.password&quot;
    kind: Secret
    name: application-secret
    valuesKey: AWESOME_SECRET
</code></pre>
<div id="admonition-example-2" class="admonition example">
<div class="admonition-title">
<p>Example</p>
<p><a class="admonition-anchor-link" href="notes/secret-variations-with-flux.html#admonition-example-2"></a></p>
</div>
<div>
<p>View example <ins><a href="https://ln.intra.falhalla.com/ARdun">Helm Release</a></ins> and corresponding <ins><a href="https://ln.intra.falhalla.com/hNef8">Secret</a></ins>.</p>
</div>
</div>
<h3 id="method-4-variable-substitution-with-flux"><a class="header" href="#method-4-variable-substitution-with-flux">Method 4: Variable Substitution with Flux</a></h3>
<blockquote>
<p><em>Flux variable substitution can inject secrets into any YAML manifest. This requires the <ins><a href="https://fluxcd.io/docs/components/kustomize/kustomization/">Flux Kustomization</a></ins> configured to enable <ins><a href="https://fluxcd.io/docs/components/kustomize/kustomization/#variable-substitution">variable substitution</a></ins>. Correctly configured this allows you to use <code>${GLOBAL_AWESOME_SECRET}</code> in any YAML manifest.</em></p>
</blockquote>
<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: cluster-secrets
  namespace: flux-system
stringData:
  GLOBAL_AWESOME_SECRET: &quot;GLOBAL SUPER SECRET VALUE&quot;
</code></pre>
<pre><code class="language-yaml">apiVersion: kustomize.toolkit.fluxcd.io/v1beta2
kind: Kustomization
# ...
spec:
# ...
  decryption:
    provider: sops
    secretRef:
      name: sops-age
  postBuild:
    substituteFrom:
      - kind: Secret
        name: cluster-secrets
</code></pre>
<div id="admonition-example-3" class="admonition example">
<div class="admonition-title">
<p>Example</p>
<p><a class="admonition-anchor-link" href="notes/secret-variations-with-flux.html#admonition-example-3"></a></p>
</div>
<div>
<p>View example <ins><a href="https://ln.intra.falhalla.com/ZMbfI">Fluxtomization</a></ins>, <ins><a href="https://ln.intra.falhalla.com/y6DJS">Helm Release</a></ins>, and corresponding <ins><a href="https://ln.intra.falhalla.com/kRoHj">Secret</a></ins>.</p>
</div>
</div>
<h2 id="final-thoughts"><a class="header" href="#final-thoughts">Final Thoughts</a></h2>
<ul>
<li>
<p>For the first <strong>three methods</strong> consider using a tool like <ins><a href="https://github.com/stakater/Reloader">stakater/reloader</a></ins> to restart the pod when the secret changes.</p>
</li>
<li>
<p>Using reloader on a pod using a secret provided by Flux Variable Substitution will lead to pods being restarted during any change to the secret while related to the pod or not.</p>
</li>
<li>
<p>The last method should be used when all other methods are not an option, or used when you have a ‚Äúglobal‚Äù secret used by a bunch of YAML manifests.</p>
</li>
</ul>
<footer id="open-on-gh"><a href="https://github.com/cfallwell/homelab-ops/edit/main/docs/src/notes/secret-variations-with-flux.md">Edit this page on GitHub</a></footer><div style="break-before: page; page-break-before: always;"></div><h1 id="yaml-madness"><a class="header" href="#yaml-madness">YAML Madness</a></h1>
<p>YAML aliases, anchors and overrides are a great way to keep your manifests DRY (<strong>D</strong>o not <strong>R</strong>epeat <strong>Y</strong>ourself) but only on a very basic level.</p>
<h2 id="anchors-and-aliases"><a class="header" href="#anchors-and-aliases">Anchors and Aliases</a></h2>
<div id="admonition-note" class="admonition note">
<div class="admonition-title">
<p>Note</p>
<p><a class="admonition-anchor-link" href="notes/yaml-madness.html#admonition-note"></a></p>
</div>
<div>
<p>The anchor operator <strong>&amp;</strong> is a way to define a variable and the alias character <strong>*</strong> is a way to reference the value defined in the anchor.</p>
</div>
</div>
<pre><code class="language-yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: &amp;app &quot;awesome-app&quot;
  namespace: default
  labels:
    app.kubernetes.io/name: *app
</code></pre>
<p><em>this will be rendered out to...</em></p>
<pre><code class="language-yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: awesome-app
  namespace: default
  labels:
    app.kubernetes.io/name: &quot;awesome-app&quot;
</code></pre>
<h2 id="overrides"><a class="header" href="#overrides">Overrides</a></h2>
<div id="admonition-note-1" class="admonition note">
<div class="admonition-title">
<p>Note</p>
<p><a class="admonition-anchor-link" href="notes/yaml-madness.html#admonition-note-1"></a></p>
</div>
<div>
<p>The <strong>&lt;&lt;</strong> operator allows referencing a block of YAML as many times as needed.</p>
</div>
</div>
<pre><code class="language-yaml">apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: &amp;app &quot;awesome-app&quot;
  namespace: default
  labels: &amp;labels
    app.kubernetes.io/instance: *app
    app.kubernetes.io/name: *app
spec:
  selector:
    matchLabels:
      &lt;&lt;: *labels
</code></pre>
<p><em>this will be rendered out to...</em></p>
<pre><code class="language-yaml">apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: &quot;awesome-app&quot;
  namespace: default
  labels:
    app.kubernetes.io/instance: &quot;awesome-app&quot;
    app.kubernetes.io/name: &quot;awesome-app&quot;
spec:
  selector:
    matchLabels:
      app.kubernetes.io/instance: &quot;awesome-app&quot;
      app.kubernetes.io/name: &quot;awesome-app&quot;
</code></pre>
<h2 id="important-notes"><a class="header" href="#important-notes">Important Notes</a></h2>
<ul>
<li>Defining an anchor, alias or override cannot be referenced in separate YAML docs whether it is in the same file or not.</li>
<li>You absolutely cannot concat, or do any advanced string functions on anchors, aliases or overrides.</li>
<li>Try to make sure your YAML is comprehensible, don't get hung up on making DRY an absolute rule to follow.</li>
</ul>
<footer id="open-on-gh"><a href="https://github.com/cfallwell/homelab-ops/edit/main/docs/src/notes/yaml-madness.md">Edit this page on GitHub</a></footer>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>






        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
